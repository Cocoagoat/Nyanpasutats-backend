{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7375e74-46fc-4245-899a-e28e1ab067aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from filenames import *\n",
    "import sys\n",
    "\n",
    "print(sys.executable)\n",
    "from general_utils import *\n",
    "from MAL_utils import *\n",
    "from tensorflow import keras\n",
    "import polars as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pyarrow.parquet as pq\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from UserDB import UserDB\n",
    "from Tags import Tags\n",
    "from AnimeDB import AnimeDB\n",
    "from AffinityDB import AffDBEntryCreator, GeneralData, User, AffinityDB\n",
    "from tensorflow.keras import regularizers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062d2074-cbce-4a7d-b6a4-963a3a8990ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_chunk(chunk):\n",
    "    # full_df = full_df.with_columns(pl.col('User Scored Shows') + full_df[\"User Scored Shows\"].mean())\n",
    "    chunk[\"Show Popularity\"] = np.log10(chunk[\"Show Popularity\"] / 2) / 1.5 - 3\n",
    "    chunk[\"User Scored Shows\"] = (np.log10(chunk[\"User Scored Shows\"]) - 2.5) * 2\n",
    "    # cols_for_norm = [\"Show Score\", \"Mean Score\", \"Standard Deviation\", \"Tag Count\",\n",
    "    #                  \"Max Pos Affinity\", \"Avg Pos Affinity\", \"Avg Affinity\"]\n",
    "    # for col in cols_for_norm:\n",
    "    #     chunk[col] = (chunk[col] - chunk[col].mean())/chunk[col].std()\n",
    "    cols_for_norm = [x for x in list(chunk.columns) if x not in\n",
    "                     [\"Pos Affinity Ratio\", \"Neg Affinity Ratio\", \"Show Popularity\", \"User Scored Shows\", \"User Score\"]]\n",
    "    for col in cols_for_norm:\n",
    "        chunk[col] = (chunk[col] - chunk[col].mean()) / chunk[col].std()\n",
    "    # chunk.to_parquet(\"TestChunk2.parquet\")\n",
    "    print(5)\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af7b2c4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def normalize_df(df=None, og_filename=None, for_predict=False, is_aff_db=False, data=None):\n",
    "    if og_filename:\n",
    "        if os.path.isfile(f\"{og_filename}-N.parquet\"):\n",
    "            # If a normalized version of a specific database we want to normalize already\n",
    "            # exists, just load it and return that, else normalize.\n",
    "            return pl.read_parquet(f\"{og_filename}-N.parquet\")\n",
    "\n",
    "    if df is None:\n",
    "        raise ValueError(\"No df and no filename were provided\")\n",
    "\n",
    "    # These two columns are normalized the same way regardless of for_predict, not with z-score.\n",
    "    df[\"Show Popularity\"] = np.log10(df[\"Show Popularity\"] / 2) / 1.5 - 3\n",
    "    df[\"User Scored Shows\"] = (np.log10(df[\"User Scored Shows\"]) - 2.5) * 2\n",
    "\n",
    "    # The other three columns excluded here should not be normalized at all.\n",
    "    cols_for_norm = [x for x in list(df.columns) if x not in\n",
    "                     [\"Pos Affinity Ratio\", \"Neg Affinity Ratio\",\n",
    "                      \"Show Popularity\", \"User Scored Shows\", \"User Score\"]]\n",
    "\n",
    "    if for_predict:\n",
    "        # If we're normalizing the database made from a user's list, we must use the main database's\n",
    "        # mean and standard deviation, as they will be different from the mini-affinity database\n",
    "        # created from the user's list.\n",
    "        if type(data) != GeneralData:\n",
    "            raise ValueError(\"Data must be sent to acquire mean of main affinity database\"\n",
    "                             \"if for_predict=True\")\n",
    "        for col in cols_for_norm:\n",
    "            df[col] = (df[col] - data.aff_db_means[col]) / data.aff_db_stds[col]\n",
    "    elif is_aff_db:\n",
    "        mean_dict={}\n",
    "        std_dict={}\n",
    "        print(\"Beginning normalization of main affinity database\")\n",
    "        # Regular database normalization\n",
    "        for col in cols_for_norm:\n",
    "            mean_dict[col] = df.loc[df[col] != 0, col].mean()\n",
    "            std_dict[col] =  df.loc[df[col] != 0, col].std()\n",
    "            df.loc[df[col] != 0, col] = (df.loc[df[col] != 0, col] - mean_dict[col]) / std_dict[col]\n",
    "        data.aff_means = mean_dict\n",
    "        data.aff_stds = std_dict\n",
    "        print(\"Finished normalization, saving means into general data for future usage\")\n",
    "        save_pickled_file(\"general_data.pickle\", data)\n",
    "        time.sleep(20)\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6163ed",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_on_chunk(model, size, chunk_dataset):\n",
    "    train_size = int(size*train_ratio)\n",
    "\n",
    "    train_dataset = chunk_dataset.take(train_size)\n",
    "    test_dataset = chunk_dataset.skip(train_size)\n",
    "\n",
    "    train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size)\n",
    "    test_dataset = test_dataset.shuffle(buffer_size).batch(batch_size)\n",
    "\n",
    "    print(\"Obtained train and test\")\n",
    "\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        epochs=50,  # Number of times the entire dataset is passed forward and backward through the neural network\n",
    "        validation_data=test_dataset,\n",
    "        verbose=1,  # To view the training progress for each epoch\n",
    "        callbacks = [callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca98724-5103-49bc-a30a-c1afa2590b54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54ed919-9d36-4ae1-b81c-bf7f1b942786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aff_db = pq.ParquetFile(f\"{aff_db_filename}.parquet\")\n",
    "# user_db = UserDB()\n",
    "# num_training_examples = aff_db.metadata.num_rows\n",
    "# num_features = len(aff_db.schema.names) - 1\n",
    "# print(num_training_examples, num_features)\n",
    "# num_labels = 10\n",
    "\n",
    "\n",
    "\n",
    "# buffer_size = 1000000\n",
    "# batch_size = 2048\n",
    "# train_ratio = 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e8c6cf-d7b6-460d-a3ad-d43d2ff7e277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421d390a-3478-438f-adf0-1106945b71fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk = pd.read_parquet(\"TestChunk.parquet\")\n",
    "\n",
    "# if not os.path.isfile(f\"{aff_db_filename}-N.parquet\"):\n",
    "#     aff_db = pd.read_parquet(f\"{aff_db_filename}.parquet\")\n",
    "#     time.sleep(20)\n",
    "#     print(\"Filling NaNs\")\n",
    "#     aff_db = aff_db.fillna(0)\n",
    "#     time.sleep(20)\n",
    "#     print(\"Normalizing\")\n",
    "#     data = load_pickled_file(\"general_data.pickle\")\n",
    "#     aff_db = normalize_df(aff_db, for_predict=False, is_aff_db=True, data=data)\n",
    "#     # cols_to_drop = [x for x in chunk.columns if x.startswith(\"Themes\")]\n",
    "#     # chunk.drop(cols_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "#     print(aff_db.head(10))\n",
    "    \n",
    "#     time.sleep(20) # Let it load in peace so it doesn't blow up your PC\n",
    "#     # NOTE - IF ANYONE ELSE IS USING THIS FOR SOME BIZARRE REASON - SPLIT THE DB INTO SEVERAL PARTS IF YOU DON'T HAVE AT LEAST 32GB OF RAM.\n",
    "#     # Otherwise your PC will crash in seconds. If you have only 32GB, make sure nothing else aside from browsers is running, otherwise same goes for you.\n",
    "#     # 48GB and above can run this safely with no issues.\n",
    "    \n",
    "#     aff_db.to_parquet(f\"{aff_db_filename}-N.parquet\")\n",
    "# else:\n",
    "#     print(\"Reading database\")\n",
    "#     aff_db = pd.read_parquet(f\"{aff_db_filename}-N-2M5.parquet\")\n",
    "#     # aff_db = aff_db.drop('Show Score', axis=1)\n",
    "\n",
    "# features = aff_db.iloc[:, :-1].values\n",
    "# labels = aff_db.iloc[:, -1].values.reshape(-1, 1)\n",
    "# print(\"Obtaining tf.Dataset\")\n",
    "# chunk_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d0683f-df1a-4b3e-8504-a866993de52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_file2(file_path):\n",
    "    # Read a single parquet file using PyArrow\n",
    "    df = pq.read_parquet(file_path)\n",
    "    # return tf.data.Dataset.from_tensor_slices(table.to_pandas().to_dict(orient='series'))\n",
    "    labels = df['User Score']\n",
    "    features = df.drop(columns=['User Score'])\n",
    "\n",
    "    return features, labels\n",
    "    \n",
    "def read_parquet_file(file_path, batch_size):\n",
    "    # Read a single parquet file using PyArrow\n",
    "    df = pq.read_parquet(file_path)\n",
    "    labels = df['User Score']\n",
    "    features = df.drop(columns=['User Score'])\n",
    "    total_rows = len(df)\n",
    "\n",
    "    # Iterate through the DataFrame in chunks of size batch_size\n",
    "    for i in range(0, total_rows, batch_size):\n",
    "        batch_features = features.iloc[i: i + batch_size]\n",
    "        batch_labels = labels.iloc[i: i + batch_size]\n",
    "        yield batch_features, batch_labels\n",
    "\n",
    "def read_and_decode(file_path):\n",
    "    # Read and decode a single file\n",
    "    return read_parquet_file(file_path)\n",
    "    \n",
    "\n",
    "def train_on_chunk2(model):\n",
    "\n",
    "    def count_major_parts():\n",
    "        p = 1\n",
    "        while True:\n",
    "            if not os.path.exists(f\"Partials\\\\{aff_db_filename}-P{p}-S.parquet\"):\n",
    "                break\n",
    "            p += 1\n",
    "\n",
    "        return p-1\n",
    "\n",
    "    batch_size = 2048\n",
    "\n",
    "    file_amount = count_major_parts()\n",
    "\n",
    "    file_paths = [f\"{aff_db_filename}-P{i+1}-S\" for i in range(file_amount)]\n",
    "\n",
    "    files = tf.data.Dataset.list_files(file_paths)\n",
    "\n",
    "    train_files = files.take(file_amount-1)\n",
    "    test_files = files.skip(file_amount-1)\n",
    "    \n",
    "    # train_dataset = train_files.map(read_parquet_file)\n",
    "    # test_dataset = test_files.map(read_parquet_file)\n",
    "\n",
    "    train_dataset = train_files.interleave(\n",
    "        lambda file_path: tf.data.Dataset.from_generator(\n",
    "            read_parquet_file, args=[file_path], output_signature=(tf.TensorSpec(shape=(batch_size, number_of_features), dtype=tf.float32),\n",
    "                  tf.TensorSpec(shape=(batch_size,), dtype=tf.int32))\n",
    "        ),\n",
    "        cycle_length=1\n",
    "    )\n",
    "    test_dataset = test_files.interleave(\n",
    "        lambda file_path: tf.data.Dataset.from_generator(\n",
    "            read_parquet_file, args=[file_path], output_signature=(tf.float32, tf.int32)\n",
    "        ),\n",
    "        cycle_length=1\n",
    "    )\n",
    "\n",
    "    # train_dataset = train_files.map(lambda file_path : tf.py_function(read_parquet_file, [file_path], (tf.float32, tf.int32)))\n",
    "    # test_dataset = test_files.map(lambda file_path : tf.py_function(read_parquet_file, [file_path], (tf.float32, tf.int32)))\n",
    "\n",
    "    train_dataset = train_dataset.batch(batch_size)\n",
    "    test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        epochs=100,  # Number of times the entire dataset is passed forward and backward through the neural network\n",
    "        validation_data=test_dataset,\n",
    "        verbose=1,  # To view the training progress for each epoch\n",
    "        callbacks = [callback]\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4de0e7-540e-4e7d-a7e9-fda01f15da97",
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_db = pq.ParquetFile(f\"{aff_db_filename}-P1-S.parquet\")\n",
    "# num_training_examples = aff_db.metadata.num_rows\n",
    "\n",
    "# num_training_examples = len(aff_db)\n",
    "num_features = len(aff_db.columns) - 1\n",
    "# print(num_training_examples, num_features)\n",
    "# buffer_size = 1000000\n",
    "# batch_size = 2048\n",
    "# train_ratio = 0.95\n",
    "\n",
    "\n",
    "# file_amount = count_major_parts()\n",
    "\n",
    "# file_paths = [f\"{aff_db_filename}-P{i}\" for i in range(1,18)]\n",
    "\n",
    "# files = tf.data.Dataset.list_files(file_paths)\n",
    "\n",
    "# train_files = files.take(16)\n",
    "# test_files = files.skip(16)\n",
    "\n",
    "# dataset = files.map(read_parquet_file)\n",
    "\n",
    "# dataset = files.interleave(\n",
    "#     read_and_decode,\n",
    "#     cycle_length=len(file_paths),\n",
    "#     num_parallel_calls=1\n",
    "# )\n",
    "\n",
    "\n",
    "print(\"Starting model creation\")\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(num_features, input_shape=(num_features,)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dense(64),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dense(64),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mean_absolute_error',\n",
    "              metrics=['mean_absolute_error'])\n",
    "\n",
    "# print(aff_db.head(20))\n",
    "print(\"Initiating sleep\")\n",
    "\n",
    "time.sleep(20)\n",
    "print(\"Beginning training\")\n",
    "# train_on_chunk(model, num_training_examples, chunk_dataset)\n",
    "train_on_chunk2(model)\n",
    "model.save('my_model3.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57e7430-dff4-483f-98d7-9815e9441fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_scores(user_name, model):\n",
    "    aff_db = AffinityDB()\n",
    "    tags = Tags()\n",
    "    # features = AffinityDBFeatures()\n",
    "    data = load_pickled_file(\"general_data.pickle\")\n",
    "    aff_db_dict = AffDBEntryCreator.initialize_aff_db_dict(tags)\n",
    "    user_db = UserDB()\n",
    "    user_row = user_db.get_user_db_entry(user_name)\n",
    "    relevant_shows = list(tags.entry_tags_dict.keys())\n",
    "    user = User(user_name, scores=user_row.select(relevant_shows).row(0, named=True), scored_amount = user_row[\"Scored Shows\"][0])\n",
    "    aff_db_entry_creator = AffDBEntryCreator(user, data, tags)\n",
    "    aff_db_entry_creator.create_db_entries_from_user_list()\n",
    "    user_db_entries = aff_db_entry_creator.aff_db_entry_dict\n",
    "\n",
    "    user_shows_df = pd.DataFrame(user_db_entries)\n",
    "    normalized_df = normalize_df(user_shows_df)\n",
    "    features = normalized_df.iloc[:, :-1].values\n",
    "    # print(features)\n",
    "    print(model.predict(features))\n",
    "    print(user_row)\n",
    "\n",
    "my_model = tf.keras.models.load_model('my_model.h5')\n",
    "predict_scores(\"BaronBrixius\", my_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2698b5-fc2e-42d1-a5f7-072afbc30b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9214e0db-1f8a-4d52-aacc-0940590ff5ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3af600-7883-48dc-9d73-51706bdb7bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MALenv39_kernel",
   "language": "python",
   "name": "malenv39_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
